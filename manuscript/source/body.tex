\section{Introduction}
Autonomous systems (AS) are pervasive in current society and set to become even more so with current technological growth trends and adoption rates. Systems with embedded artificial intelligence (AI) and machine learning (ML) algorithms can be found in numerous applications from mobile phones~\cite{medium_ai_phones}, insurance pricing~\cite{kuo2020towards} and vacuum cleaners~\cite{tf_vacuum} to medical diagnostics~\cite{kononenko2001machine}, detecting structural damage to buildings~\cite{avci2021review} and predicting the shape of protein molecules~\cite{alpha_fold} to name a few. 
%
There is also growing use of machine learning in a range of safety-critical systems (SCSs), for example in the aerospace and automotive industry where low reliability of these systems could result in catastrophic failure and potentially loss of life or damage to property and the environment. These safety-critical autonomous systems (SCAS) present a complex but essential challenge to the safety assurance and verification community. 
%
Conventional V\&V is principally concerned with assessing the system against a set of requirements, providing guarantees of functionality and assurance of safety. But if autonomous systems are to be fully accepted into society, there must be acknowledgement of and evidence to show compliance with a broad range of \emph{trustworthiness qualities}. 
%
This paper focuses on a reviewing what trustworthiness means for AS, how AS can be verified as trustworthy, and the challenges associated with what that verification process may look like. 

A widely held tenet is that there can never be a suitable amount of verification for complex, autonomous systems that gives complete assurance [ref] and thinking must now move beyond verification at the system design stage to a more continuous operational evaluation, e.g. runtime verification [ref] which also brings other unresolved issues such suitable oracle design [ref]. 
%
Further to this issue, is the lack of standards by which many trustworthy qualities should be appraised and the methods by which they should be evaluated.


%
To present a defensible safety argument for AS and SCAS...
trust stakeholders
\emph{trust qualities}
application specific
What human factors are considered eg. ISO29119


In addition to assessing the AS trustworthiness, there must also be consideration to gain, calibrate and maintain user trust in the system~\cite{kok2020trust, Chiou2021}, else failures related to overtrust and undertrust are possible. 


%\subsection{Document Structure}
In the following, related work is reviewed in Section

\section{Related Work}\label{Related_work}


\section{Assessment Framework Vision}\label{Assessment_Framework_Vision}


\subsection{Assessment Methods \& Corroborative Evidence}
Gaining reliability assurance of SCASs using testing alone is unfeasible given the often high-dimensional operational state space. Multiple testing methodologies should be employed where appropriate, e.g. verification, falsification and testing, [Harper Corroborative 2022] combining mutually consistent evidence from multiple and diverse assessment methods will raise the confidence in system trustworthiness.

Knowledge of the internal state of the system is often hidden, e.g. blackbox, due to IP and commercial sensitivity, but whitebox access will be essential for certain aspects of trustworthiness assessment. This may not need to reveal sensitive algorithms but just enough information through observability points in the software architecture could go a long way to understanding if automated decisions are made for the right reason\cite{koopman2018toward}. 



\section{Conclusion}\label{conclusion}
